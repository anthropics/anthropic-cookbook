{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Managed LLM Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [Getting Started](#getting-started)\n",
    "- [Memory Implementations](#memory-implementations)\n",
    "    - [Basic Memory](#implementation-1-simple-memory-tool)\n",
    "    - [Compactify Memory](#implementation-2-compactify-memory)\n",
    "    - [File-Based Memory](#implementation-3-file-based-memory)\n",
    "- [Basic Evaluations](#basic-evaluations)\n",
    "- [Future Work](#future-work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Managing memory effectively is a critical part of building agents and agentic workflows that handle long-horizon tasks. In this cookbook we demonstrate a few different strategies for \"self-managed\" (LLM-managed) memory. Use this notebook as a starting point for your own memory implementations. We do not expect that memory tools are one-size-fits-all, and further believe that different domains/tasks necessarily lend themselves to more or less rigid memory scaffolding. The Claude 4 model family has proven to be particularly strong at utilizing [memory tooling](https://www.anthropic.com/news/claude-4#:~:text=more%20on%20methodology.-,Model%20improvements,-In%20addition%20to), and we're excited to see how teams extend the ideas below.\n",
    "\n",
    "\n",
    "#### Why do we need to manage memory?\n",
    "\n",
    "LLMs have finite context windows (200k tokens for Claude 4 Sonnet & Opus). This means that for any request, if the sum of prompt tokens and output tokens exceeds the model’s context window, the system will return a validation error. As many teams building with LLMs quickly learn, there is additional complexity in identifying and working within the *effective* [context window](https://docs.anthropic.com/en/docs/build-with-claude/context-windows) of an LLM. See our tips for [long context prompting](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/long-context-tips) to learn more about effective context windows and best practices.\n",
    "\n",
    "In addition to the above, memory is important for the following reasons:\n",
    "-  **Long context windows are computationally expensive:** Attention mechanisms scale quadratically—doubling context length quadruples compute cost. Most tasks only need a small fraction of available context, making it wasteful to process millions of irrelevant tokens. This is why humans don't memorize entire textbooks; we take notes and build mental models instead.\n",
    "- **More efficient processing:** When LLMs write and maintain their own notes—saving successful strategies, key insights, and relevant context—they're effectively updating their capabilities in real-time without retraining. Models that excel at these operations can maintain coherent behavior over extremely long time horizons while using only a fraction of the computational resources required for full context windows.\n",
    "\n",
    "Successfully building LLM-based systems is an exercise in discarding the unnecessary tokens and efficiently storing + retrieving the relevant tokens for the task at-hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install deps\n",
    "%pip install -q -U anthropic python-dotenv nest_asyncio PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env setup\n",
    "from anthropic import Anthropic\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# api key must be in .env file in project\n",
    "load_dotenv()\n",
    "if os.getenv(\"ANTHROPIC_API_KEY\") is None:\n",
    "    raise ValueError(\"ANTHROPIC_API_KEY not found in .env file\")\n",
    "\n",
    "client = Anthropic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Clone the agents quickstart implementation</b>\n",
    "\n",
    "We are going to use some of the core work from the agents quickstart implementation which can be found [here](https://github.com/anthropics/anthropic-quickstarts/tree/main/agents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository already exists at /tmp/anthropic-quickstarts\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import os\n",
    "\n",
    "# Check if the repo already exists\n",
    "if not os.path.exists('/tmp/anthropic-quickstarts'):\n",
    "    # Clone the agents quickstart implementation\n",
    "    !git clone https://github.com/anthropics/anthropic-quickstarts.git /tmp/anthropic-quickstarts\n",
    "else:\n",
    "    print(\"Repository already exists at /tmp/anthropic-quickstarts\")\n",
    "\n",
    "# IMPORTANT: Insert at the beginning of sys.path to override any existing 'agents' modules\n",
    "if '/tmp/anthropic-quickstarts' not in sys.path:\n",
    "    sys.path.insert(0, '/tmp/anthropic-quickstarts')\n",
    "\n",
    "# Clear any cached imports of 'agents' module\n",
    "if 'agents' in sys.modules:\n",
    "    del sys.modules['agents']\n",
    "if 'agents.agent' in sys.modules:\n",
    "    del sys.modules['agents.agent']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Confirm the agents repo import works as expected.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*eye roll* Another laptop crisis. What's it doing? Singing off-key? Refusing to work unless you feed it cookies? Details, please.\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from agents.agent import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"MyAgent\",\n",
    "    system=\"You are an extremely cynical, snarky, and quick-witted customer support agent. Provide short responses to user queries.\",\n",
    ")\n",
    "\n",
    "response = agent.run(\"I'm having issues with my laptop. Can you help me?\")\n",
    "print(response.content[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation 1: Simple Memory Tool\n",
    "\n",
    "*This implementation is a reflection of our agents quickstarts repo [here](https://github.com/anthropics/anthropic-quickstarts/tree/main/agents/tools). For more information on tool use, see the Anthropic API tools [docs](https://docs.anthropic.com/en/docs/build-with-claude/tool-use/overview).*\n",
    "\n",
    "The `SimpleMemory()` tool gives the model a scratchpad to manage memory. This is maintained as a single string that can be read or updated.\n",
    "\n",
    "Here we've defined the `read`, `write`, and `edit` actions. Explicitly defining `read` means the model won't have access to the full contents of memory at every turn. We recommend that if you follow this pattern you introduce a separate, shortened summary or metadata object describing the contents of memory and include that in every request (ideally preventing excessive reads).\n",
    "\n",
    "\n",
    "<b>When would you use this?</b>\n",
    "\n",
    "You want to quickly spin up a memory experiment or augment an existing long-context task. Start here if you don't have high conviction around the types of items that need to be stored or if the agent must support many interaction types.\n",
    "\n",
    "<b><i>General Notes on Tool Use:</i></b> \n",
    "- Your tool descriptions should be clear and sufficiently detailed. The best way to guide model behavior around tools is by providing direction as to when / under what conditions tools should be used. \n",
    "- If you find that a task requires the agent or workflow manage many (~20+) tools, you may find better performance by introducing a higher level delegation step to route the task to a specialized LLM-step designed around a smaller, logically coupled subset of tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLE MEMORY TOOL\n",
    "from agents.tools.base import Tool\n",
    "\n",
    "class SimpleMemory(Tool):\n",
    "    \"\"\"String-based memory tool for storing and modifying persistent text.\n",
    "\n",
    "    This tool maintains a single in-memory string that can be read,\n",
    "    replaced, or selectively edited using string replacement. It provides safety\n",
    "    warnings when overwriting content or when edit operations would affect\n",
    "    multiple occurrences.\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"simple_memory\"\n",
    "\n",
    "    #TODO: Provide additional domain context to guide Claude on the types of items that should be stored\n",
    "    description = \"\"\"Tool for managing persistent text memory with read, write and edit operations.\n",
    "        Read: Retrieves full memory contents as a string\n",
    "        Write: Replaces entire memory (warns when overwriting existing data)\n",
    "        Edit: Performs targeted string replacement (warns on multiple matches)\"\"\"\n",
    "\n",
    "    # single tool that exposes 3 distinct abilities\n",
    "    input_schema = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"action\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"read\", \"write\", \"edit\"],\n",
    "                \"description\": \"The memory operation to perform: read retrieves current content, write replaces everything, edit performs string replacement\",\n",
    "            },\n",
    "            \"content\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Full text content to store when using write action (ignored for read/edit)\",\n",
    "            },\n",
    "            \"old_string\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Exact text to find and replace when using edit action (must be unique in memory)\",\n",
    "            },\n",
    "            \"new_string\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Replacement text to insert when using edit action\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"action\"],\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        self.full_memory = \"\"\n",
    "        self.compressed_memory = \"\" # not doing anything with this for now\n",
    "        \n",
    "    async def execute(self, **kwargs) -> str:\n",
    "        \"\"\"Execute the memory tool with provided parameters.\"\"\"\n",
    "        action = kwargs.get(\"action\")\n",
    "        content = kwargs.get(\"content\", \"\")\n",
    "        old_string = kwargs.get(\"old_string\", \"\")\n",
    "        new_string = kwargs.get(\"new_string\", \"\")\n",
    "\n",
    "        if action == \"read\":\n",
    "            return self._read_memory()\n",
    "        elif action == \"write\":\n",
    "            print(\"Writing to memory...\")\n",
    "            return self._write_memory(content)\n",
    "        elif action == \"edit\":\n",
    "            return self._edit_memory(old_string, new_string)\n",
    "        else:\n",
    "            return f\"Error: Unknown action '{action}'. Valid actions are read, write, edit.\"\n",
    "\n",
    "    def _read_memory(self) -> str:\n",
    "        \"\"\"Read the current memory contents.\"\"\"\n",
    "        return self.full_memory\n",
    "\n",
    "    def _write_memory(self, content: str) -> str:\n",
    "        \"\"\"Replace the entire memory with new content.\"\"\"\n",
    "        if self.full_memory:\n",
    "            previous = self.full_memory\n",
    "            self.full_memory = content\n",
    "            return f\"Warning: Overwriting existing content. Previous content was:\\n{previous}\\n\\nMemory has been updated successfully.\"\n",
    "        self.full_memory = content\n",
    "        return \"Memory updated successfully.\"\n",
    "\n",
    "    def _edit_memory(self, old_string: str, new_string: str) -> str:\n",
    "        \"\"\"Replace occurrences of old string with new string.\"\"\"\n",
    "        if old_string not in self.full_memory:\n",
    "            return f\"Error: '{old_string}' not found in memory.\"\n",
    "\n",
    "        old_memory = self.full_memory\n",
    "        count = old_memory.count(old_string)\n",
    "\n",
    "        if count > 1:\n",
    "            return f\"Warning: Found {count} occurrences of '{old_string}'. Please confirm which occurrence to replace or use more specific context.\"\n",
    "\n",
    "        self.full_memory = self.full_memory.replace(old_string, new_string)\n",
    "        return f\"Edited memory: 1 occurrence replaced.\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.full_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation 2: Compactify Memory \n",
    "\n",
    "Maintaining a rolling summary over long interactions is a pattern you might have already built into your application. Generally the implementation looks something like:\n",
    "\n",
    "\n",
    "1) Set a `token_threshold`. This threshold could be the context window for the model, but generally you would set it lower.\n",
    "2) Track the current token usage: `system_prompt` + `rolling_summary` (up to step_n) + `message_history[]` (since step_n)\n",
    "3) When token usage exceeds threshold, summarize using current `rolling_summary` + `message_history[]`. Clear `message_history[]` and reset `rolling_summary`. \n",
    "\n",
    "We believe the pattern outlined above works well. The modification we're introducing with this tool is <i>allowing the model</i> to invoke the summarization operation at its own discretion. You might decide to combine these ideas, allowing the model to determine when to summarize but preserve the `token_threshold` + force summarization as a fail safe in tetheh case that Claude doesn't decide to compactify memory in time. \n",
    "\n",
    "<b>When would you use this?</b>\n",
    "\n",
    "Similar to the first implementation, test this tool when you don't have a clear idea of what should be saved. Behaviorally speaking, decision making around when to condense a long running conversation can be more reliably tuned compared to the open-endedness of the first memory tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPACTIFY MEMORY TOOL\n",
    "from agents.utils.history_util import MessageHistory\n",
    "\n",
    "class CompactifyMemory(Tool):\n",
    "    \"\"\"Memory summarization tool.\n",
    "    \n",
    "    Summarizes and replaces the existing message history.\n",
    "    Expects to have access to a message_history object that is shared with the request handler.\n",
    "    Descriptions should be modified to introduce use-case specific guidance.\n",
    "    \"\"\"\n",
    "    \n",
    "    name = \"compactify_memory\"\n",
    "    description = \"\"\"The memory compactifier tool will compress the current conversation history (replaces message history entirely). \n",
    "    Should be used when there is sufficient information that requires summarization.\n",
    "    The summary should keep relevant information from any previous summaries.\n",
    "    \"\"\"\n",
    "\n",
    "    input_schema = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {},\n",
    "        \"required\": []\n",
    "    }\n",
    "        \n",
    "    def __init__(self, client: Anthropic):\n",
    "        self.client = client\n",
    "        self.full_memory = ''\n",
    "        self.compressed_memory = '' # not doing anything with this for now\n",
    "\n",
    "    def run_compactify (self, message_history: MessageHistory):\n",
    "        summary = self.client.messages.create(\n",
    "            model=\"claude-sonnet-4-20250514\",\n",
    "            max_tokens = 10000, # modify as needed\n",
    "            messages=[*message_history.messages, {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"\"\"Your task is to summarize the conversation using the previous summary as well as the messages since the last summary. Note that this will replace the previous summary entirely, so be sure to include the most relevant information that should be persisted.\"\"\"\n",
    "            }]\n",
    "        )\n",
    "\n",
    "        # modify the message history object in place\n",
    "        message_history.messages = [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Conversation Summary: \" +  summary.content[0].text\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "    async def execute(self, **kwargs) -> str:\n",
    "        # ATTN: note that we're breaking tool encapsulation here and will be executing the function outside the agent loop (see agents.agent.py)\n",
    "        # we do this because we don't have an elegant way to share message state between the agent and tool just yet (...stay tuned)\n",
    "        return \"pending_compactify\"\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.full_memory\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation 3: \"File-Based\" Memory\n",
    "\n",
    "This implementation gives Claude the ability to interact with a 'memory' system represented to the model as a hierarchical file structure. The example below implements a basic directory, where the 'files' are just strings that we've labeled as plaintext files (the '.txt' label has no impact functionally, but can be useful for behavioral consistency).\n",
    "\n",
    "Hierarchical directory structures are easily readable and well-understood by humans and LLMs alike, so it's fitting to use them as a mechanism to represent persistent state more generally to an LLM. While you can connect to and define access patterns for any external storage system, a quick way to get started is with Anthropic's new <b>[Files API](https://docs.anthropic.com/en/docs/build-with-claude/files)</b>. The Files API enables storage and retrieval of objects for use in future requests.\n",
    "\n",
    "Ideally you (the developer & domain expert) would construct an initial state for the directory structure that adequately represents your domain context. Having some pre-defined structure provides useful behavioral queues for the model, but you should also introduce more explicit guidance to guard against excessive reads / writes / new file creation / etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# HELPER FUNCTION: Parse markdown string for JSON\n",
    "def parse_markdown_json(markdown_string):\n",
    "    \"\"\"\n",
    "    Parses a JSON string from a Markdown string.\n",
    "\n",
    "    Args:\n",
    "        markdown_string (str): The Markdown string containing JSON.\n",
    "\n",
    "    Returns:\n",
    "        dict or list or None: A Python object representing the parsed JSON, or None if parsing fails.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"```(?:json)?\\n(.*?)\\n```\", markdown_string, re.DOTALL)\n",
    "    if match:\n",
    "        json_string = match.group(1).strip()\n",
    "    else:\n",
    "        json_string = markdown_string.strip()\n",
    "    try:\n",
    "        parsed_json = json.loads(json_string)\n",
    "        return parsed_json\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "# HELPER CLASS: Memory Node\n",
    "class MemoryNode:\n",
    "    def __init__(self, name, is_directory=False, parent=None, content=None):\n",
    "        self.name = name\n",
    "        self.is_directory = is_directory\n",
    "        self.parent = parent\n",
    "        self.content = content if not is_directory else None\n",
    "        self.children = {} if is_directory else None\n",
    "    \n",
    "    def add_child(self, name, is_directory=False, content=None):\n",
    "        \"\"\"Add a child node to the current node.\"\"\"\n",
    "        if not self.is_directory:\n",
    "            raise ValueError(f\"Cannot add child to file '{self.name}'\")\n",
    "        \n",
    "        if name in self.children:\n",
    "            raise ValueError(f\"Child '{name}' already exists\")\n",
    "        \n",
    "        child = MemoryNode(name, is_directory, parent=self, content=content)\n",
    "        self.children[name] = child\n",
    "        return child\n",
    "    \n",
    "    def remove_child(self, name):\n",
    "        \"\"\"Remove a child node from the current node.\"\"\"\n",
    "        if not self.is_directory:\n",
    "            raise ValueError(f\"Cannot remove child from file '{self.name}'\")\n",
    "            \n",
    "        if name not in self.children:\n",
    "            raise ValueError(f\"Child '{name}' not found\")\n",
    "        \n",
    "        del self.children[name]\n",
    "    \n",
    "    def find(self, path):\n",
    "        \"\"\"Find a node by path (ex: 'folder1/folder2/file.txt').\"\"\"\n",
    "        if not path:\n",
    "            return self\n",
    "        \n",
    "        parts = path.strip('/').split('/', 1)\n",
    "        child_name = parts[0]\n",
    "        \n",
    "        if not self.is_directory or child_name not in self.children:\n",
    "            return None\n",
    "            \n",
    "        child = self.children[child_name]\n",
    "        \n",
    "        if len(parts) == 1:\n",
    "            return child\n",
    "        else:\n",
    "            return child.find(parts[1])\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"MemoryNode(name='{self.name}', is_directory={self.is_directory})\"\n",
    "\n",
    "# HELPER CLASS: Memory Tree\n",
    "class MemoryTree:\n",
    "    def __init__(self):\n",
    "        self.root = MemoryNode(\"memory\", is_directory=True)\n",
    "\n",
    "    def add(self, path, content):\n",
    "        \"\"\"Add content to a node at the given path (ex: 'folder1/folder2/file.txt').\"\"\"\n",
    "        node = self.root.find(path)\n",
    "        if node:\n",
    "            node.content = content\n",
    "        else:\n",
    "            raise ValueError(f\"Path '{path}' not found\")\n",
    "\n",
    "    def get(self, path):\n",
    "        \"\"\"Get content from a node at the given path.\"\"\"\n",
    "        node = self.root.find(path)\n",
    "        if node:\n",
    "            return node.content\n",
    "        else:\n",
    "            raise ValueError(f\"Path '{path}' not found\")\n",
    "\n",
    "    def edit(self, path, content):\n",
    "        node = self.root.find(path)\n",
    "        if node:\n",
    "            node.content = content\n",
    "        else:\n",
    "            raise ValueError(f\"Path '{path}' not found\")\n",
    "\n",
    "    def _build_from_json_recursive(self, json_obj, parent_node):\n",
    "        \"\"\"Recursively build the tree from a JSON object.\"\"\"\n",
    "\n",
    "        # handle root memory (already initialized)\n",
    "        if len(json_obj) == 1 and 'memory' in json_obj:\n",
    "            json_obj = json_obj['memory']\n",
    "\n",
    "        for name, value in json_obj.items():\n",
    "            if isinstance(value, dict):\n",
    "                # Create a directory node\n",
    "                child_node = parent_node.add_child(name, is_directory=True)\n",
    "                self._build_from_json_recursive(value, child_node)\n",
    "            else:\n",
    "                # Create a file node with content\n",
    "                parent_node.add_child(name, content=value)\n",
    "\n",
    "    def build_from_json_string(self, str_json_obj):\n",
    "        json_obj = parse_markdown_json(str_json_obj)\n",
    "        self._build_from_json_recursive(json_obj, self.root)\n",
    "\n",
    "    def print_tree(self, node=None, prefix=''):\n",
    "        \"\"\"Print a directory tree structure.\"\"\"\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "        \n",
    "        # Build list of children for proper indexing\n",
    "        children = list(node.children.items()) if node.is_directory else []\n",
    "        \n",
    "        for index, (name, child) in enumerate(children):\n",
    "            is_last = index == len(children) - 1\n",
    "            \n",
    "            # Create the appropriate connector\n",
    "            if prefix == '' and node == self.root:\n",
    "                # For root level items (direct children of root)\n",
    "                connector = '└── ' if is_last else '├── '\n",
    "                self.lines.append(f\"{connector}{name}\")\n",
    "                \n",
    "                # Recurse if this is a directory\n",
    "                if child.is_directory:\n",
    "                    extension = '    ' if is_last else '│   '\n",
    "                    self.print_tree(child, extension)\n",
    "            else:\n",
    "                # For non-root level items\n",
    "                connector = '└── ' if is_last else '├── '\n",
    "                self.lines.append(f\"{prefix}{connector}{name}\")\n",
    "                \n",
    "                # Recurse if this is a directory\n",
    "                if child.is_directory:\n",
    "                    extension = '    ' if is_last else '│   '\n",
    "                    self.print_tree(child, prefix + extension)\n",
    "\n",
    "    def get_tree(self):\n",
    "        \"\"\"Return the tree as a string.\"\"\"\n",
    "        self.lines = []\n",
    "        \n",
    "        # Start with the root directory name\n",
    "        self.lines.append(self.root.name)\n",
    "\n",
    "        # Print the rest of the tree\n",
    "        self.print_tree()\n",
    "        return '\\n'.join(self.lines)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.get_tree()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'file',\n",
       "  'id': 'file_011CPaGpXxdBojQLTszA5LGp',\n",
       "  'size_bytes': 544347,\n",
       "  'created_at': '2025-05-28T16:51:06.716000Z',\n",
       "  'filename': 'sample.pdf',\n",
       "  'mime_type': 'application/pdf',\n",
       "  'downloadable': False},\n",
       " {'type': 'file',\n",
       "  'id': 'file_011CPYNG2Sf1cWjuCFhKJFV7',\n",
       "  'size_bytes': 3,\n",
       "  'created_at': '2025-05-27T16:41:15.335000Z',\n",
       "  'filename': 'number.txt',\n",
       "  'mime_type': 'text/plain',\n",
       "  'downloadable': True}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import mimetypes\n",
    "\n",
    "# HELPER CLASS FOR FILE STORAGE using the new files API!\n",
    "class StorageManager:\n",
    "    def __init__(self, api_key):\n",
    "        if api_key is None:\n",
    "            raise ValueError(\"ANTHROPIC_API_KEY not available.\")\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://api.anthropic.com/v1/files\"\n",
    "        self.headers = {\n",
    "            \"x-api-key\": self.api_key,\n",
    "            \"anthropic-version\": \"2023-06-01\",\n",
    "            \"anthropic-beta\": \"files-api-2025-04-14\"\n",
    "        }\n",
    "\n",
    "    def _execute_request(self, method, endpoint, data=None, files=None):\n",
    "        \"\"\"Execute a request to the API.\"\"\"\n",
    "        url = f\"{self.base_url}/{endpoint}\"\n",
    "\n",
    "        res = requests.request(method, url, headers=self.headers, data=data, files=files)\n",
    "        if res.status_code == 200:\n",
    "            return res.json()\n",
    "        else:\n",
    "            raise ValueError(f\"Request failed: {res.status_code} - {res.text}\")\n",
    "\n",
    "    def list_files(self):\n",
    "        \"\"\"List all files. Direct curl request to the API.\"\"\"\n",
    "        res = requests.get(\n",
    "            self.base_url,\n",
    "            headers=self.headers\n",
    "        )\n",
    "        if res.status_code != 200:\n",
    "            raise ValueError(f\"Failed to retrieve files: {res.status_code} - {res.text}\")\n",
    "        res = res.json()\n",
    "        return res['data']\n",
    "        \n",
    "        \n",
    "    def get_file_metadata(self, file_id):\n",
    "        \"\"\"Get a file by ID. Direct curl request to the API.\"\"\"\n",
    "        res = requests.get(\n",
    "            f\"{self.base_url}/{file_id}\",\n",
    "            headers=self.headers\n",
    "        )\n",
    "        if res.status_code != 200:\n",
    "            raise ValueError(f\"Failed to retrieve file: {res.status_code} - {res.text}\")\n",
    "        res = res.json()\n",
    "        return res \n",
    "        \n",
    "    def upload_file(self, file_path):\n",
    "        \"\"\"Upload a file to the API.\"\"\"        \n",
    "        # Determine the file's MIME type\n",
    "        mime_type, _ = mimetypes.guess_type(file_path)\n",
    "        if mime_type is None:\n",
    "            mime_type = \"application/octet-stream\"  # Fallback to binary if type unknown\n",
    "        \n",
    "        with open(file_path, \"rb\") as file_obj:\n",
    "            files = {\n",
    "                \"file\": (os.path.basename(file_path), file_obj, mime_type)\n",
    "            }\n",
    "            \n",
    "            res = requests.post(\n",
    "                self.base_url,\n",
    "                headers=self.headers,\n",
    "                files=files\n",
    "            )\n",
    "            \n",
    "        if res.status_code == 200:\n",
    "            return res.json()\n",
    "        else:\n",
    "            raise ValueError(f\"Failed to upload file: {res.status_code} - {res.text}\")\n",
    "        \n",
    "# example usage\n",
    "#file_path = \"/Users/user/Downloads/SB1029-ProjectUpdate-FINAL_020317-A11Y.pdf\" # REPLACE\n",
    "storage_manager = StorageManager(os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "#uploaded = storage_manager.upload_file(file_path)\n",
    "#storage_manager.get_file_metadata(uploaded['id'])\n",
    "storage_manager.list_files()[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does this look like in practice?\n",
    "\n",
    "Imagine you want to build a company wide chatbot that needs access to information about ongoing projects, teams, customers, etc. You could build a retrieval pipeline that chunks, loads, and refreshes your company documents within a vector database, but tuning this pipeline is a non-trivial task. The neat part about building file-based memory scaffolding for this problem is you can treat files managed by your organization in the exact same manner as files managed by the LLM (just with different read/write permissions).\n",
    "\n",
    "Imagine the agent has access the following directory at every turn and can read and update these objects at its discretion.\n",
    "\n",
    "```\n",
    "claude_memories/\n",
    "├── user_session_notes/\n",
    "│   ├── cli_debuggin_session_2025_05_02.txt\n",
    "│   ├── quarterly_planning_2025_05_01.txt\n",
    "│   └── data_analysis_2025_05_01.txt\n",
    "├── general_preferences/\n",
    "│   ├── code_style.txt\n",
    "│   └── all_preferences.txt\n",
    "files/\n",
    "├── projects/\n",
    "│   ├── building_agi.txt\n",
    "│   └── prompt_optimization.txt\n",
    "├── documents/\n",
    "│   ├── updated_risk_report.txt\n",
    "│   ├── company_strategy.txt\n",
    "│   └── 2024_annual_report.txt\n",
    "├── teams/\n",
    "│   ├── engineering.txt\n",
    "│   └── marketing.txt\n",
    "├── customers/\n",
    "│   ├── acme.txt\n",
    "│   └── widgets.txt\n",
    "```\n",
    "\n",
    "A fully featured implementation might enforce the following:\n",
    "- `claude_memories/` directory (llm-managed) allows <b>read</b> & <b>write</b> operations\n",
    "- `user_session_notes` is stored + loaded <b>per user</b>\n",
    "- `files/` directory (org-managed) is <b>read-only</b> and connects to an external storage system\n",
    "- as directories grow past a certain size you may want to limit traversal up to depth <b><i>n</i></b>, and then allow the model to invoke deeper traversal only as neeeded\n",
    "\n",
    "*In theory, the Simple Memory tool presented in #1 could be represented as a file system with a single available path.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memory\n",
       "├── self_managed\n",
       "│   ├── user_session_notes\n",
       "│   │   ├── ongoing_projects.txt\n",
       "│   │   └── preferences.txt\n",
       "│   └── projects\n",
       "│       └── building_agi.txt\n",
       "└── files\n",
       "    └── projects"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example usage\n",
    "company_agent_memory = MemoryTree()\n",
    "\n",
    "# example of the type of object you might get from an LLM (if you wanted to allow the LLM to construct it's own memory structure)\n",
    "example_str = \"\"\"\n",
    "```json\n",
    "{\"self_managed\": {\"user_session_notes\":{\"ongoing_projects.txt\":\"I should remember that the user is working on prompt optimization\",\"preferences.txt\":\"I should remember that the user prefers to be called Jimbo\"},\"projects\":{\"building_agi.txt\":\"I should remember that the user is working on building AGI\"}}, \"files\": {\"projects\":\"building_agi.txt\"}}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "company_agent_memory.build_from_json_string(example_str)\n",
    "company_agent_memory\n",
    "\n",
    "# test out the file utilities below\n",
    "\n",
    "# print(company_agent_memory)\n",
    "# print(\"GET:\", company_agent_memory.get('self_managed/user_session_notes/ongoing_projects.txt'))\n",
    "# company_agent_memory.edit('self_managed/user_session_notes/ongoing_projects.txt', 'The user gave up on prompt optimization')\n",
    "# print(\"UPDATED:\", company_agent_memory.get('self_managed/user_session_notes/ongoing_projects.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE BASED MEMORY TOOL\n",
    "\n",
    "class FileBasedMemoryTool(Tool):\n",
    "    \"\"\"\n",
    "    Manage memory as a nested file system. This is specifically designed around the new files API.\n",
    "\n",
    "    This tool provides a simple interace for interacting with this memory system.\n",
    "    We have only defined three actions: GET, EDIT, and BUILD. In practice, you likely would opt for a more opinionated file structure \n",
    "    and more fine-grained control over access to the memory. We will rely on the default message truncation mechanism of the request handler.\n",
    "    \"\"\"\n",
    "\n",
    "    name = 'hierarchical_memory'\n",
    "    description = 'Interact with file system for storing memories, retrieving memories, and rebuilding the memory state.'\n",
    "    input_schema = {\n",
    "        'type': 'object',\n",
    "        'properties': {\n",
    "            'action': {\n",
    "                'type': 'string',\n",
    "                'enum': ['get', 'edit', 'build']\n",
    "            },\n",
    "            'paths': {\n",
    "                'type': 'array',\n",
    "                'items': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'Path to the memory item'\n",
    "                },\n",
    "                'description': 'List of paths for the associated action. Available with GET and EDIT actions. (GET can have multiple paths, EDIT should have one path)'\n",
    "            },\n",
    "            'content': {\n",
    "                'type': 'string',\n",
    "                'description': 'Content that will be written to the specified path. Only available with the EDIT action.'\n",
    "            },\n",
    "            'new_memory_object': {\n",
    "                'type': 'object',\n",
    "                'description': 'Full memory output object to rebuild the memory scaffold. Only available with the BUILD action. This should be a JSON object representing the desired tree structure for memories. The values should be None (as a placeholder for future content).'\n",
    "            }\n",
    "        },\n",
    "        'required': ['action']\n",
    "    }\n",
    "    \n",
    "    def __init__(self, storage_manager: StorageManager):\n",
    "        self.full_memory = MemoryTree()\n",
    "        self.compressed_memory = self.full_memory # including the compressed memory for standardizing the interface\n",
    "        self.storage_manager = storage_manager\n",
    "\n",
    "    async def execute(self, **kwargs) -> str:\n",
    "        action = kwargs.get('action')\n",
    "        paths = kwargs.get('paths')\n",
    "        content = kwargs.get('content')\n",
    "        new_memory_object = kwargs.get('new_memory_object')\n",
    "\n",
    "        if action == 'get':\n",
    "            # we need to build the file messages from the file metadata (https://docs.anthropic.com/en/docs/docs/build-with-claude/files)\n",
    "            message_refs = [{\"type\": \"document\", \"source\": { \"type\": \"file\", \"file_id\": self.full_memory.get(path)}} for path in paths]\n",
    "            return message_refs\n",
    "\n",
    "        elif action == 'edit':\n",
    "            path = paths[0]\n",
    "\n",
    "            #create txt file in tmp dir with content\n",
    "            with open(f'/tmp/{path}.txt', 'w') as f:\n",
    "                f.write(content)\n",
    "\n",
    "            # upload the file to the API\n",
    "            uploaded = self.storage_manager.upload_file(f'/tmp/{path}.txt')\n",
    "\n",
    "            # add the file to the memory tree (using the id)\n",
    "            self.full_memory.edit(path, uploaded['id'])\n",
    "            return 'Updated'\n",
    "        \n",
    "        elif action == 'build':\n",
    "            self.full_memory.build_from_json_string(new_memory_object)\n",
    "            return 'Updated'\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Invalid action: {action}\")\n",
    "        \n",
    "    def __str__(self):\n",
    "        return str(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Memory Management Advice:\n",
    "- Maintain a summary or compressed representation of memory preloaded in the context, even if your tools require actions from the model to load the full information.\n",
    "\n",
    "- Encourage the model to reason about what to remember and how to update its memory content given the domain or task at hand.\n",
    "\n",
    "- Encourage the model to keep the content of its memory up-to-date and coherent. Discourage excessive file creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Interactive Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "import datetime\n",
    "import textwrap\n",
    "from typing import List\n",
    "from anthropic import Anthropic\n",
    "\n",
    "memory_tools = [\n",
    "    SimpleMemory(),\n",
    "    CompactifyMemory(client),\n",
    "    FileBasedMemoryTool()\n",
    "]\n",
    "\n",
    "def process_memory_function(agent, tool):\n",
    "    \"\"\"Because some memory tools work with the agents message history object\"\"\"\n",
    "    mem_tool_names = [tool.name for tool in memory_tools]\n",
    "    for tool in agent.tools:\n",
    "        if tool.name in mem_tool_names:\n",
    "            # ATTN: bit of a hack, but we need to inject some additional functionality\n",
    "            if tool.name == 'compactify_memory':\n",
    "                tool.run_compactify(self.agent.message_history)\n",
    "        \n",
    "\n",
    "class ChatInterface:\n",
    "    def __init__(self, agent: Agent, max_line_length=80):\n",
    "        self.max_line_length = max_line_length\n",
    "        self.agent = agent\n",
    "        self.messages = [] # managing the window's messages separately from the Agent's messages\n",
    "        self.memory = ''\n",
    "\n",
    "        # Chat history container\n",
    "        self.chat_output = widgets.Output(layout=widgets.Layout(\n",
    "            height='400px', \n",
    "            overflow='auto',\n",
    "            border='1px solid #ccc',\n",
    "            padding='10px',\n",
    "            display='flex',\n",
    "            flex_flow='wrap-reverse'\n",
    "        ))\n",
    "        \n",
    "        # Text input for new messages\n",
    "        self.text_input = widgets.Text(\n",
    "            placeholder='Type your message here...',\n",
    "            layout=widgets.Layout(width='100%')\n",
    "        )\n",
    "        \n",
    "        # Send button\n",
    "        self.send_button = widgets.Button(\n",
    "            description='Send',\n",
    "            button_style='primary'\n",
    "        )\n",
    "        \n",
    "        # Memory settings display\n",
    "        self.memory_display = widgets.Output(layout=widgets.Layout(\n",
    "            width='100%', \n",
    "            height='400px',\n",
    "            border='1px solid #ccc',\n",
    "            padding ='10px',\n",
    "\n",
    "        ))  \n",
    "        \n",
    "        # Input container (text input + send button)\n",
    "        input_box = widgets.HBox([\n",
    "            self.text_input,\n",
    "            self.send_button\n",
    "        ], layout=widgets.Layout(width='100%'))\n",
    "        \n",
    "        # Left panel (chat)\n",
    "        left_panel = widgets.VBox([\n",
    "            widgets.Label('Chat'),\n",
    "            self.chat_output,\n",
    "            input_box\n",
    "        ], layout=widgets.Layout(\n",
    "            width='50%',\n",
    "            padding='10px',\n",
    "        ))\n",
    "        \n",
    "        # Right panel (memory settings)\n",
    "        right_panel = widgets.VBox([\n",
    "            widgets.Label('Memory'),\n",
    "            self.memory_display\n",
    "        ], layout=widgets.Layout(\n",
    "            width='50%',\n",
    "            padding='10px'\n",
    "        ))\n",
    "        \n",
    "        # Main layout\n",
    "        self.interface = widgets.HBox([\n",
    "            left_panel,\n",
    "            right_panel\n",
    "        ], layout=widgets.Layout(\n",
    "            width='100%',\n",
    "            display='flex'\n",
    "        ))\n",
    "        \n",
    "        # Event handlers\n",
    "        self.send_button.on_click(self.on_send)\n",
    "        self.text_input.on_submit(self.on_send)\n",
    "        \n",
    "        # Message history\n",
    "        self.messages = []\n",
    "    \n",
    "    def on_send(self, _):\n",
    "        \"\"\"Handle sending a message\"\"\"\n",
    "        message = self.text_input.value.strip()\n",
    "        if message:\n",
    "            self.add_message(\"user\", message)\n",
    "            self.text_input.value = \"\"\n",
    "\n",
    "            # call the agent with the message\n",
    "            response = self.agent.run(message)\n",
    "            self.add_message(\"assistant\", response.content[0].text)\n",
    "\n",
    "            ## PROCESS\n",
    "\n",
    "            self.update_memory_display()\n",
    "    \n",
    "    def wrap_text(self, text):\n",
    "        \"\"\"Wrap text to fit within max_line_length\"\"\"\n",
    "        # Use textwrap to wrap long lines\n",
    "        wrapped_lines = []\n",
    "        for line in text.split('\\n'):\n",
    "            if len(line) > self.max_line_length:\n",
    "                # Wrap this line\n",
    "                wrapped = textwrap.fill(line, width=self.max_line_length)\n",
    "                wrapped_lines.append(wrapped)\n",
    "            else:\n",
    "                wrapped_lines.append(line)\n",
    "        return '\\n'.join(wrapped_lines)\n",
    "    \n",
    "    def add_message(self, role, message):\n",
    "        \"\"\"Add a message to the chat history with text wrapping\"\"\"\n",
    "        timestamp = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
    "        # Wrap the message text\n",
    "        wrapped_message = self.wrap_text(message)\n",
    "        \n",
    "        self.messages.append({\n",
    "            \"role\": role,\n",
    "            \"content\": message,  # Store original message\n",
    "            \"wrapped_message\": wrapped_message,  # Store wrapped version\n",
    "            \"timestamp\": timestamp\n",
    "        })\n",
    "        \n",
    "        with self.chat_output:\n",
    "            clear_output()\n",
    "            # Display all messages with HTML formatting\n",
    "            for msg in self.messages:\n",
    "                if msg['role'] == 'user':\n",
    "                    color = '#0066cc'\n",
    "                else:\n",
    "                    color = '#000000'\n",
    "                    \n",
    "                display(HTML(\n",
    "                    f\"<div style='margin-bottom: 10px; color: {color};'>\"\n",
    "                    f\"<strong>{msg['role']} [{msg['timestamp']}]:</strong> \"\n",
    "                    f\"{msg['wrapped_message']}\"\n",
    "                    f\"</div>\"\n",
    "                ))\n",
    "            \n",
    "    def update_memory_display(self):\n",
    "        \"\"\"Update the memory display with current memory content\"\"\"\n",
    "        with self.memory_display:\n",
    "            clear_output()\n",
    "            display(HTML(f\"<pre style='margin: 10px; padding: 0; white-space: pre-wrap;'>{self.memory}</pre>\"))\n",
    "    \n",
    "    def display(self):\n",
    "        \"\"\"Display the interface\"\"\"\n",
    "        return self.interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run The Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_tool = FileBasedMemoryTool() # or SimpleMemory() or CompactifyMemory(client) or FileBasedMemoryTool(storage_manager)\n",
    "model_config = {\n",
    "    \"model\": \"claude-sonnet-4-20250514\",\n",
    "}\n",
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    system=\"You are a helpful assistant designed to work with a user.\", # additional memory instructions can be added here\n",
    "    tools=[memory_tool],\n",
    "    config=model_config,\n",
    ")\n",
    "\n",
    "chat = ChatInterface(\n",
    "    agent=agent,\n",
    ")\n",
    "\n",
    "chat.display()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
