{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestrator-Workers Workflow\n",
    "In this workflow, a central LLM dynamically breaks down tasks, delegates them to worker LLMs, and synthesizes their results.\n",
    "\n",
    "### When to use this workflow\n",
    "This workflow is well-suited for complex tasks where you can't predict the subtasks needed. The key difference from simple parallelization is its flexibilityâ€”subtasks aren't pre-defined, but determined by the orchestrator based on the specific input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional\n",
    "from util import llm_call, extract_xml\n",
    "\n",
    "def parse_tasks(tasks_xml: str) -> List[Dict]:\n",
    "    \"\"\"Parse XML tasks into a list of task dictionaries.\"\"\"\n",
    "    tasks = []\n",
    "    current_task = {}\n",
    "    \n",
    "    for line in tasks_xml.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        if line.startswith(\"<task>\"):\n",
    "            current_task = {}\n",
    "        elif line.startswith(\"<type>\"):\n",
    "            current_task[\"type\"] = line[6:-7].strip()\n",
    "        elif line.startswith(\"<description>\"):\n",
    "            current_task[\"description\"] = line[12:-13].strip()\n",
    "        elif line.startswith(\"</task>\"):\n",
    "            if \"description\" in current_task:\n",
    "                if \"type\" not in current_task:\n",
    "                    current_task[\"type\"] = \"default\"\n",
    "                tasks.append(current_task)\n",
    "    \n",
    "    return tasks\n",
    "\n",
    "class FlexibleOrchestrator:\n",
    "    \"\"\"Break down tasks and run them in parallel using worker LLMs.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        orchestrator_prompt: str,\n",
    "        worker_prompt: str,\n",
    "    ):\n",
    "        \"\"\"Initialize with prompt templates.\"\"\"\n",
    "        self.orchestrator_prompt = orchestrator_prompt\n",
    "        self.worker_prompt = worker_prompt\n",
    "\n",
    "    def _format_prompt(self, template: str, **kwargs) -> str:\n",
    "        \"\"\"Format a prompt template with variables.\"\"\"\n",
    "        try:\n",
    "            return template.format(**kwargs)\n",
    "        except KeyError as e:\n",
    "            raise ValueError(f\"Missing required prompt variable: {e}\")\n",
    "\n",
    "    def process(self, task: str, context: Optional[Dict] = None) -> Dict:\n",
    "        \"\"\"Process task by breaking it down and running subtasks in parallel.\"\"\"\n",
    "        context = context or {}\n",
    "        \n",
    "        # Step 1: Get orchestrator response\n",
    "        orchestrator_input = self._format_prompt(\n",
    "            self.orchestrator_prompt,\n",
    "            task=task,\n",
    "            **context\n",
    "        )\n",
    "        orchestrator_response = llm_call(orchestrator_input)\n",
    "        \n",
    "        # Parse orchestrator response\n",
    "        analysis = extract_xml(orchestrator_response, \"analysis\")\n",
    "        tasks_xml = extract_xml(orchestrator_response, \"tasks\")\n",
    "        tasks = parse_tasks(tasks_xml)\n",
    "        \n",
    "        print(\"\\n=== ORCHESTRATOR OUTPUT ===\")\n",
    "        print(f\"\\nANALYSIS:\\n{analysis}\")\n",
    "        print(f\"\\nTASKS:\\n{tasks}\")\n",
    "        \n",
    "        # Step 2: Process each task\n",
    "        worker_results = []\n",
    "        for task_info in tasks:\n",
    "            worker_input = self._format_prompt(\n",
    "                self.worker_prompt,\n",
    "                original_task=task,\n",
    "                task_type=task_info['type'],\n",
    "                task_description=task_info['description'],\n",
    "                **context\n",
    "            )\n",
    "            \n",
    "            worker_response = llm_call(worker_input)\n",
    "            result = extract_xml(worker_response, \"response\")\n",
    "            \n",
    "            worker_results.append({\n",
    "                \"type\": task_info[\"type\"],\n",
    "                \"description\": task_info[\"description\"],\n",
    "                \"result\": result\n",
    "            })\n",
    "            \n",
    "            print(f\"\\n=== WORKER RESULT ({task_info['type']}) ===\\n{result}\\n\")\n",
    "        \n",
    "        return {\n",
    "            \"analysis\": analysis,\n",
    "            \"worker_results\": worker_results,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Use Case: Marketing Variation Generation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "ORCHESTRATOR_PROMPT = \"\"\"\nAnalyze this task and break it down into 2-3 distinct approaches:\n\nTask: {task}\nTarget Audience: {target_audience}\nKey Features: {key_features}\n\nReturn your response in this format:\n\n<analysis>\nExplain your understanding of the task and which variations would be valuable.\nFocus on how each approach serves different aspects of the task.\n</analysis>\n\n<tasks>\n    <task>\n    <type>formal</type>\n    <description>Write a precise, technical version that emphasizes specifications</description>\n    </task>\n    <task>\n    <type>conversational</type>\n    <description>Write an engaging, friendly version that connects with readers</description>\n    </task>\n</tasks>\n\"\"\"\n\nWORKER_PROMPT = \"\"\"\nGenerate content based on:\nTask: {original_task}\nStyle: {task_type}\nGuidelines: {task_description}\nTarget Audience: {target_audience}\nKey Features: {key_features}\n\nReturn your response in this format:\n\n<response>\nYour content here, maintaining the specified style and fully addressing requirements.\n</response>\n\"\"\"\n\n\norchestrator = FlexibleOrchestrator(\n    orchestrator_prompt=ORCHESTRATOR_PROMPT,\n    worker_prompt=WORKER_PROMPT,\n)\n\nresults = orchestrator.process(\n    task=\"Write a product description for a new eco-friendly water bottle\",\n    context={\n        \"target_audience\": \"environmentally conscious millennials\",\n        \"key_features\": [\"plastic-free\", \"insulated\", \"lifetime warranty\"]\n    })"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}